{
    "sourceFile": "tools/train_net.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1674835081920,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1674835081920,
            "name": "Commit-0",
            "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nr\"\"\"\nBasic training script for PyTorch\n\"\"\"\n\n# Set up custom environment before nearly anything else is imported\n# NOTE: this should be the first import (no not reorder)\nfrom maskrcnn_benchmark.utils.env import setup_environment  # noqa F401 isort:skip\n\nimport argparse\nimport os\n\nimport torch\nfrom maskrcnn_benchmark.config import cfg\nfrom maskrcnn_benchmark.data import make_data_loader\nfrom maskrcnn_benchmark.solver import make_lr_scheduler\nfrom maskrcnn_benchmark.solver import make_optimizer\nfrom maskrcnn_benchmark.engine.inference import inference\nfrom maskrcnn_benchmark.engine.trainer import do_train\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\nfrom maskrcnn_benchmark.utils.collect_env import collect_env_info\nfrom maskrcnn_benchmark.utils.comm import synchronize, get_rank\nfrom maskrcnn_benchmark.utils.imports import import_file\nfrom maskrcnn_benchmark.utils.logger import setup_logger\nfrom maskrcnn_benchmark.utils.miscellaneous import mkdir, save_config\n\n# See if we can use apex.DistributedDataParallel instead of the torch default,\n# and enable mixed-precision via apex.amp\ntry:\n    from apex import amp\nexcept ImportError:\n    raise ImportError('Use APEX for multi-precision via apex.amp')\n\n\ndef train(cfg, local_rank, distributed):\n    model = build_detection_model(cfg)\n    device = torch.device(cfg.MODEL.DEVICE)\n    model.to(device)\n\n    optimizer = make_optimizer(cfg, model)\n    scheduler = make_lr_scheduler(cfg, optimizer)\n\n    # Initialize mixed-precision training\n    use_mixed_precision = cfg.DTYPE == \"float16\"\n    amp_opt_level = 'O1' if use_mixed_precision else 'O0'\n    model, optimizer = amp.initialize(\n        model, optimizer, opt_level=amp_opt_level)\n\n    if distributed:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[local_rank], output_device=local_rank,\n            # this should be removed if we update BatchNorm stats\n            broadcast_buffers=False,\n        )\n\n    arguments = {}\n    arguments[\"iteration\"] = 0\n\n    output_dir = cfg.OUTPUT_DIR\n\n    save_to_disk = get_rank() == 0\n    checkpointer = DetectronCheckpointer(\n        cfg, model, optimizer, scheduler, output_dir, save_to_disk\n    )\n    extra_checkpoint_data = checkpointer.load(cfg.MODEL.WEIGHT)\n    arguments.update(extra_checkpoint_data)\n\n    source_data_loader = make_data_loader(\n        cfg,\n        is_train=True,\n        is_distributed=distributed,\n        # start_iter=arguments[\"iteration\"],\n        is_source=True\n    )\n\n    target_data_loader = make_data_loader(\n        cfg,\n        is_train=True,\n        is_distributed=distributed,\n        # start_iter=arguments[\"iteration\"],\n        is_source=False,\n    )\n\n    # test_period = cfg.SOLVER.TEST_PERIOD\n    checkpoint_period = cfg.SOLVER.CHECKPOINT_PERIOD\n    da_enable = cfg.MODEL.DOMAIN_ADAPTION.ENABLE_DA\n\n    do_train(\n        cfg,\n        model,\n        source_data_loader,\n        target_data_loader,\n        da_enable,\n        optimizer,\n        scheduler,\n        checkpointer,\n        device,\n        checkpoint_period,\n        arguments,\n    )\n\n    return model\n\n\ndef run_test(cfg, model, distributed):\n    if distributed:\n        model = model.module\n    torch.cuda.empty_cache()  # TODO check if it helps\n    iou_types = (\"bbox\",)\n    if cfg.MODEL.MASK_ON:\n        iou_types = iou_types + (\"segm\",)\n    if cfg.MODEL.KEYPOINT_ON:\n        iou_types = iou_types + (\"keypoints\",)\n    output_folders = [None] * len(cfg.DATASETS.TEST)\n    dataset_names = cfg.DATASETS.TEST\n    if cfg.OUTPUT_DIR:\n        for idx, dataset_name in enumerate(dataset_names):\n            output_folder = os.path.join(\n                cfg.OUTPUT_DIR, \"inference\", dataset_name)\n            mkdir(output_folder)\n            output_folders[idx] = output_folder\n    data_loaders_val = make_data_loader(\n        cfg, is_train=False, is_distributed=distributed)\n    for output_folder, dataset_name, data_loader_val in zip(output_folders, dataset_names, data_loaders_val):\n        inference(\n            model,\n            data_loader_val,\n            dataset_name=dataset_name,\n            iou_types=iou_types,\n            box_only=False if cfg.MODEL.RETINANET_ON else cfg.MODEL.RPN_ONLY,\n            bbox_aug=cfg.TEST.BBOX_AUG.ENABLED,\n            device=cfg.MODEL.DEVICE,\n            expected_results=cfg.TEST.EXPECTED_RESULTS,\n            expected_results_sigma_tol=cfg.TEST.EXPECTED_RESULTS_SIGMA_TOL,\n            output_folder=output_folder,\n        )\n        synchronize()\n\n\ndef main():\n    parser = argparse.ArgumentParser(\n        description=\"PyTorch Object Detection Training\")\n    parser.add_argument(\n        \"--config-file\",\n        default=\"configs/uda_nuclei_seg/e2e_mask_rcnn_R_101_FPN_1x_gn_consep2tnbc.yaml\",\n        metavar=\"FILE\",\n        help=\"path to config file\",\n        type=str,\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=0)\n    parser.add_argument(\n        \"--skip-test\",\n        dest=\"skip_test\",\n        help=\"Do not test the final model\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\n        \"opts\",\n        help=\"Modify config options using the command-line\",\n        default=None,\n        nargs=argparse.REMAINDER,\n    )\n\n    args = parser.parse_args()\n\n    num_gpus = int(os.environ[\"WORLD_SIZE\"]\n                   ) if \"WORLD_SIZE\" in os.environ else 1\n    args.distributed = num_gpus > 1\n\n    if args.distributed:\n        torch.cuda.set_device(args.local_rank)\n        torch.distributed.init_process_group(\n            backend=\"nccl\", init_method=\"env://\"\n        )\n        synchronize()\n\n    cfg.merge_from_file(args.config_file)\n    cfg.merge_from_list(args.opts)\n    cfg.freeze()\n\n    output_dir = cfg.OUTPUT_DIR\n    if output_dir:\n        mkdir(output_dir)\n\n    logger = setup_logger(\"maskrcnn_benchmark\", output_dir, get_rank())\n    logger.info(\"Using {} GPUs\".format(num_gpus))\n    logger.info(args)\n\n    logger.info(\"Collecting env info (might take some time)\")\n    logger.info(\"\\n\" + collect_env_info())\n\n    logger.info(\"Loaded configuration file {}\".format(args.config_file))\n    with open(args.config_file, \"r\") as cf:\n        config_str = \"\\n\" + cf.read()\n        logger.info(config_str)\n    logger.info(\"Running with config:\\n{}\".format(cfg))\n\n    output_config_path = os.path.join(cfg.OUTPUT_DIR, 'config.yml')\n    logger.info(\"Saving config into: {}\".format(output_config_path))\n    # save overloaded model config in the output directory\n    save_config(cfg, output_config_path)\n\n    model = train(cfg, args.local_rank, args.distributed)\n\n\nif __name__ == \"__main__\":\n    main()\n"
        }
    ]
}
{
    "sourceFile": "inference/cell_predictor.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1674907581100,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1674907581100,
            "name": "Commit-0",
            "content": "# Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\nimport cv2\nimport torch\nfrom torchvision import transforms as T\n\nfrom maskrcnn_benchmark.modeling.detector import build_detection_model\nfrom maskrcnn_benchmark.utils.checkpoint import DetectronCheckpointer\nfrom maskrcnn_benchmark.structures.image_list import to_image_list\nfrom maskrcnn_benchmark.modeling.roi_heads.mask_head.inference import Masker\nimport tifffile as tiff\n\nimport logging\n\nimport numpy as np\n\n\nlogger = logging.getLogger(__name__)\n\n# This one is used to load cell data as coco style when showing the final mask\n\n\nclass CellDemo(object):\n    # COCO categories for pretty print\n    CATEGORIES = [\n        \"__background\",\n        \"nuclei\",\n    ]\n\n    def __init__(\n        self,\n        cfg,\n        confidence_threshold=0.7,\n        show_mask_heatmaps=False,\n        masks_per_dim=2,\n        min_image_size=224,\n        weight=None,\n        model=None,\n    ):\n        self.cfg = cfg.clone()\n        #self.cfg = cfg\n        #self.model = build_detection_model(cfg)\n        self.model = model\n        self.model.eval()\n        self.device = torch.device(cfg.MODEL.DEVICE)\n        self.model.to(self.device)\n        self.min_image_size = min_image_size\n        self.weight = weight\n\n        if self.weight is None:\n            self.weight = cfg.MODEL.WEIGHT\n\n        checkpointer = DetectronCheckpointer(cfg, self.model)\n        _ = checkpointer.load(self.weight)\n\n        self.transforms = self.build_transform()\n\n        mask_threshold = -1 if show_mask_heatmaps else 0.5\n        self.masker = Masker(threshold=mask_threshold, padding=1)\n\n        # used to make colors for each class\n        self.palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n\n        self.cpu_device = torch.device(\"cpu\")\n        self.confidence_threshold = confidence_threshold\n        self.show_mask_heatmaps = show_mask_heatmaps\n        self.masks_per_dim = masks_per_dim\n\n    def build_transform(self):\n        \"\"\"\n        Creates a basic transformation that was used to train the models\n        \"\"\"\n        cfg = self.cfg\n\n        # we are loading images with OpenCV, so we don't need to convert them\n        # to BGR, they are already! So all we need to do is to normalize\n        # by 255 if we want to convert to BGR255 format, or flip the channels\n        # if we want it to be in RGB in [0-1] range.\n        if cfg.INPUT.TO_BGR255:\n            to_bgr_transform = T.Lambda(lambda x: x * 255)\n        else:\n            to_bgr_transform = T.Lambda(lambda x: x[[2, 1, 0]])\n\n        normalize_transform = T.Normalize(\n            mean=cfg.INPUT.PIXEL_MEAN, std=cfg.INPUT.PIXEL_STD\n        )\n\n        transform = T.Compose(\n            [\n                T.ToPILImage(),\n                T.Resize(self.min_image_size),\n                T.ToTensor(),\n                to_bgr_transform,\n                normalize_transform,\n            ]\n        )\n        return transform\n\n    def run_on_opencv_image(self, image):\n        \"\"\"\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n\n        Returns:\n            prediction (BoxList): the detected objects. Additional information\n                of the detection properties can be found in the fields of\n                the BoxList via `prediction.fields()`\n        \"\"\"\n        predictions = self.compute_prediction(image)\n        top_predictions = self.select_top_predictions(predictions)\n\n        result = image.copy()\n        if self.show_mask_heatmaps:\n            return self.create_mask_montage(result, top_predictions)\n        result = self.overlay_boxes(result, top_predictions)\n        if self.cfg.MODEL.MASK_ON:\n            result, mask_list = self.overlay_mask(result, top_predictions)\n\n        return result, mask_list\n\n    def compute_prediction(self, original_image):\n        \"\"\"\n        Arguments:\n            original_image (np.ndarray): an image as returned by OpenCV\n\n        Returns:\n            prediction (BoxList): the detected objects. Additional information\n                of the detection properties can be found in the fields of\n                the BoxList via `prediction.fields()`\n        \"\"\"\n        # apply pre-processing to image\n        image = self.transforms(original_image)\n        # convert to an ImageList, padded so that it is divisible by\n        # cfg.DATALOADER.SIZE_DIVISIBILITY\n        image_list = to_image_list(\n            image, self.cfg.DATALOADER.SIZE_DIVISIBILITY)\n        image_list = image_list.to(self.device)\n        # compute predictions\n        with torch.no_grad():\n            predictions = self.model(image_list)\n\n        predictions = [o.to(self.cpu_device) for o in predictions]\n\n        # always single image is passed at a time\n        prediction = predictions[0]\n\n        # reshape prediction (a BoxList) into the original image size\n        height, width = original_image.shape[:-1]\n        prediction = prediction.resize((width, height))\n\n        if prediction.has_field(\"mask\"):\n            # if we have masks, paste the masks in the right position\n            # in the image, as defined by the bounding boxes\n            masks = prediction.get_field(\"mask\")\n            masks = self.masker(masks, prediction)\n            prediction.add_field(\"mask\", masks)\n        return prediction\n\n    def select_top_predictions(self, predictions):\n        \"\"\"\n        Select only predictions which have a `score` > self.confidence_threshold,\n        and returns the predictions in descending order of score\n\n        Arguments:\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `scores`.\n\n        Returns:\n            prediction (BoxList): the detected objects. Additional information\n                of the detection properties can be found in the fields of\n                the BoxList via `prediction.fields()`\n        \"\"\"\n        scores = predictions.get_field(\"scores\")\n        keep = torch.nonzero(scores > self.confidence_threshold).squeeze(1)\n        predictions = predictions[keep]\n        scores = predictions.get_field(\"scores\")\n        _, idx = scores.sort(0, descending=True)\n        return predictions[idx]\n\n    def compute_colors_for_labels(self, labels):\n        \"\"\"\n        Simple function that adds fixed colors depending on the class\n        \"\"\"\n        colors = labels[:, None] * self.palette\n        colors = (colors % 255).numpy().astype(\"uint8\")\n        return colors\n\n    def overlay_boxes(self, image, predictions):\n        \"\"\"\n        Adds the predicted boxes on top of the image\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `labels`.\n        \"\"\"\n        labels = predictions.get_field(\"labels\")\n        boxes = predictions.bbox\n\n        colors = self.compute_colors_for_labels(labels).tolist()\n\n        for box, color in zip(boxes, colors):\n            box = box.to(torch.int64)\n            top_left, bottom_right = box[:2].tolist(), box[2:].tolist()\n            image = cv2.rectangle(\n                image, tuple(top_left), tuple(bottom_right), tuple(color), 1\n            )\n\n        return image\n\n    def overlay_mask(self, image, predictions):\n        \"\"\"\n        Adds the instances contours for each predicted object.\n        Each label has a different color.\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `mask` and `labels`.\n        \"\"\"\n        mask_list = predictions.get_field(\n            \"mask\").squeeze(1).numpy().transpose(1, 2, 0)\n\n        composite = image\n\n        return composite, mask_list\n\n    def create_mask_montage(self, image, predictions):\n        \"\"\"\n        Create a montage showing the probability heatmaps for each one one of the\n        detected objects\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `mask`.\n        \"\"\"\n        masks = predictions.get_field(\"mask\")\n        masks_per_dim = self.masks_per_dim\n        masks = torch.nn.functional.interpolate(\n            masks.float(), scale_factor=1 / masks_per_dim\n        ).byte()\n        height, width = masks.shape[-2:]\n        max_masks = masks_per_dim ** 2\n        masks = masks[:max_masks]\n        # handle case where we have less detections than max_masks\n        if len(masks) < max_masks:\n            masks_padded = torch.zeros(\n                max_masks, 1, height, width, dtype=torch.uint8)\n            masks_padded[: len(masks)] = masks\n            masks = masks_padded\n        masks = masks.reshape(masks_per_dim, masks_per_dim, height, width)\n        result = torch.zeros(\n            (masks_per_dim * height, masks_per_dim * width), dtype=torch.uint8\n        )\n        for y in range(masks_per_dim):\n            start_y = y * height\n            end_y = (y + 1) * height\n            for x in range(masks_per_dim):\n                start_x = x * width\n                end_x = (x + 1) * width\n                result[start_y:end_y, start_x:end_x] = masks[y, x]\n        return cv2.applyColorMap(result.numpy(), cv2.COLORMAP_JET)\n\n    def overlay_class_names(self, image, predictions):\n        \"\"\"\n        Adds detected class names and scores in the positions defined by the\n        top-left corner of the predicted bounding box\n\n        Arguments:\n            image (np.ndarray): an image as returned by OpenCV\n            predictions (BoxList): the result of the computation by the model.\n                It should contain the field `scores` and `labels`.\n        \"\"\"\n        scores = predictions.get_field(\"scores\").tolist()\n        labels = predictions.get_field(\"labels\").tolist()\n        labels = [self.CATEGORIES[i] for i in labels]\n        boxes = predictions.bbox\n\n        template = \"{}: {:.2f}\"\n        for box, score, label in zip(boxes, scores, labels):\n            x, y = box[:2]\n            s = template.format(label, score)\n            cv2.putText(\n                image, s, (x, y), cv2.FONT_HERSHEY_SIMPLEX, .5, (255,\n                                                                 255, 255), 1\n            )\n\n        return image\n"
        }
    ]
}
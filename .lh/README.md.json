{
    "sourceFile": "README.md",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 2,
            "patches": [
                {
                    "date": 1675588330336,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1675588376228,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,12 +39,11 @@\n git clone https://github.com/NVIDIA/apex.git\n cd apex\n python setup.py install --cuda_ext --cpp_ext\n \n-# install PyTorch Detection\n+# install PyTorch Detection benchmark\n cd $INSTALL_DIR\n-git clone https://github.com/facebookresearch/maskrcnn-benchmark.git\n-cd maskrcnn-benchmark\n+maskrcnn-\n \n # the following will install the lib with\n # symbolic links, so that you can modify\n # the files if you want and won't need to\n"
                },
                {
                    "date": 1675588386370,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -39,11 +39,11 @@\n git clone https://github.com/NVIDIA/apex.git\n cd apex\n python setup.py install --cuda_ext --cpp_ext\n \n-# install PyTorch Detection benchmark\n+# install PyTorch Detection maskrcnn-benchmark\n+\n cd $INSTALL_DIR\n-maskrcnn-\n \n # the following will install the lib with\n # symbolic links, so that you can modify\n # the files if you want and won't need to\n"
                }
            ],
            "date": 1675588330336,
            "name": "Commit-0",
            "content": "# PDAM: Unsupervised Domain Adaptive Instance Segmentation in Microscopy Images\n\n\nIn this project, we proposed a Panoptic Domain Adaptive Mask R-CNN (PDAM) for unsupervised instance segmentation in microscopy images.\n\n\n\nThe implementations are for our previous two papers:\n\n[Unsupervised Instance Segmentation in Microscopy Images via Panoptic Domain Adaptation and Task Re-Weighting](https://openaccess.thecvf.com/content_CVPR_2020/papers/Liu_Unsupervised_Instance_Segmentation_in_Microscopy_Images_via_Panoptic_Domain_Adaptation_CVPR_2020_paper.pdf), CVPR 2020.\n \n[PDAM: A Panoptic-Level Feature Alignment Framework for Unsupervised Domain Adaptive Instance Segmentation in Microscopy Images](https://ieeexplore.ieee.org/abstract/document/9195030), IEEE Transactions on Medical Imaging.\n\n\n\n## Introduction and Installation\n\n<!-- Please follow [maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark) to set up the environment. In this project, the Pytorch Version 1.4.0 and CUDA 10.1 are used.\n -->\npytorch version 1.10.0 and CUDA 11.3\n\n```bash\nexport INSTALL_DIR=$PWD\n\n# install pycocotools\ncd $INSTALL_DIR\ngit clone https://github.com/cocodataset/cocoapi.git\ncd cocoapi/PythonAPI\npython setup.py build_ext install\n\n# install cityscapesScripts\ncd $INSTALL_DIR\ngit clone https://github.com/mcordts/cityscapesScripts.git\ncd cityscapesScripts/\npython setup.py build_ext install\n\n# install apex\ncd $INSTALL_DIR\ngit clone https://github.com/NVIDIA/apex.git\ncd apex\npython setup.py install --cuda_ext --cpp_ext\n\n# install PyTorch Detection\ncd $INSTALL_DIR\ngit clone https://github.com/facebookresearch/maskrcnn-benchmark.git\ncd maskrcnn-benchmark\n\n# the following will install the lib with\n# symbolic links, so that you can modify\n# the files if you want and won't need to\n# re-build it\npython setup.py build develop\n\n\nunset INSTALL_DIR\n```\n\n## Data\n\n### Data Introduction\n\nIn this work, we use four datasets:\n\nHistopathology Images: TCGA-Kumar, and TNBC. Please download them from [link](https://drive.google.com/drive/folders/1l55cv3DuY-f7-JotDN7N5nbNnjbLWchK).\n\nFor the testing images in TCGA-Kumar dataset, we rename them in the inference and evaluation process. Please refer to [Link](https://cloudstor.aarnet.edu.au/plus/s/Tpd3d6H2XxUlkl4) for details.\n\nFluorescence Microscopy Images: BBC039 Version 1. Download from this [link](https://bbbc.broadinstitute.org/BBBC039).\n\nElectron Microscopy Images: [EPFL](https://www.epfl.ch/labs/cvlab/data/data-em/), and [VNC](https://github.com/unidesigner/groundtruth-drosophila-vnc).\n\n**If you are using these datasets in your research, please also remember to cite their original work.**\n\n### Data preparation\n\nAll the data should be put in `./dataset`. For the detailed path of each dataset, please refer to:\n\n`./maskrcnn_benchmark/config/path_catalog.py`\n\nHere we provide some sample images on adaptation from BBBC039V1 to TCGA-Kumar (fluo2tcga).\n\nNote that the instance annotations are stored in .json files following the MSCOCO format. If you want to generate the annotations by yourself, please follow this [repository](https://github.com/waspinator/pycococreator).\n\n## Model training\n\nFirst, follow our paper to generate synthesized patches using [CycleGAN](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix).\n\nNext, implement the nuclei inpainting mechanism by running `python auxiliary_nuclei_inpaint.py`. We have several demo results in `./nuclei_inpaint`.\n\n\nFor training the model on three UDA settings in our papers, please refer to:\n\n`./train_gn_pdam.sh`.\n\n## Model inference and Evaluation\n\nThe code for this part is in `./inference`. Just list the settings from BBBC039V1 to TCGA-Kumar as an example:\n\nTo get the instance segmentation prediction, run `python fluo2tcga_infer.py`. Remember to manually set the path of the pre-trained weights, testing images, and output folder.\n\nTo evaluate the segmentation performance under AJI, pixel-f1, and Panoptic Quality (PQ), please run `python fluo2tcga_eva.py`. The overall results for all the testing images will be saved in a .xls file.\n\nTo visualize the instance-level mask annotations/predictions, please run `python color_instance.py`.\n\n## Citations (Bibtex)\nPlease consider citing our papers in your publications if they are helpful to your research:\n```\n@inproceedings{liu2020unsupervised,\n  title={Unsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting},\n  author={Liu, Dongnan and Zhang, Donghao and Song, Yang and Zhang, Fan and O'Donnell, Lauren and Huang, Heng and Chen, Mei and Cai, Weidong},\n  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},\n  pages={4243--4252},\n  year={2020}\n}\n\n```\n\n```\n@article{liu2020pdam,\n  title={PDAM: A Panoptic-level Feature Alignment Framework for Unsupervised Domain Adaptive Instance Segmentation in Microscopy Images},\n  author={Liu, Dongnan and Zhang, Donghao and Song, Yang and Zhang, Fan and Oâ€™Donnell, Lauren and Huang, Heng and Chen, Mei and Cai, Weidong},\n  journal={IEEE Transactions on Medical Imaging},\n  year={2020},\n  publisher={IEEE}\n}\n\n\n```\n\n \n## Thanks to the Third Party Repositories\n\n[maskrcnn-benchmark](https://github.com/facebookresearch/maskrcnn-benchmark)\n\n[pytorch-CycleGAN-and-pix2pix](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix)\n\n[quip_cnn_segmentation](https://github.com/SBU-BMI/quip_cnn_segmentation)\n\n[hover_net](https://github.com/vqdang/hover_net)\n\n\n\n## Contact\n\nPlease contact Dongnan Liu (dongnanliu0201@gmail.com) for any questions.\n\n\n## License\n\nPDAM is released under the MIT license. See [LICENSE](LICENSE) for additional details.\n\n"
        }
    ]
}